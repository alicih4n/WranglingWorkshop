{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 3: Data Engineering & EDA with Python, SQL, and Pandas\n",
                "\n",
                "**Student Name:** Ali Cihan Ozdemir\n",
                "**Student ID:** 9091405\n",
                "**Group Partner:** Roshan\n",
                "\n",
                "## Objective\n",
                "This notebook demonstrates a complete data engineering pipeline: connecting to a cloud PostgreSQL database (Neon), extracting raw \"dirty\" data, cleaning and transforming it using Pandas, and performing advanced exploratory data analysis (EDA) and visualization.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Data Extraction & Inspection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import psycopg2\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sqlalchemy import create_engine\n",
                "\n",
                "# Set aesthetic style\n",
                "sns.set_style(\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "\n",
                "# Database Configuration\n",
                "DB_URL = \"postgresql://neondb_owner:npg_a2cfwEmDp5ig@ep-noisy-grass-ai9zgc4l-pooler.c-4.us-east-1.aws.neon.tech/neondb?sslmode=require\"\n",
                "\n",
                "def get_data_from_db():\n",
                "    \"\"\"Fetches data from the Neon cloud database directly into a Pandas DataFrame.\"\"\"\n",
                "    try:\n",
                "        # Using psycopg2 for the connection, but reading directly with pandas\n",
                "        conn = psycopg2.connect(DB_URL)\n",
                "        query = \"SELECT * FROM employees\"\n",
                "        df = pd.read_sql_query(query, conn)\n",
                "        conn.close()\n",
                "        return df\n",
                "    except Exception as e:\n",
                "        print(f\"Error fetching data: {e}\")\n",
                "        return pd.DataFrame()\n",
                "\n",
                "# Load the data\n",
                "df_raw = get_data_from_db()\n",
                "df = df_raw.copy() # Work on a copy\n",
                "\n",
                "print(f\"Data Loaded Successfully. Shape: {df.shape}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Data Cleaning & Wrangling\n",
                "\n",
                "### Data Collection & Cleaning Narrative\n",
                "In a real-world scenario, data often comes from legacy systems or manual entry, leading to inconsistencies. Our initial inspection (`isnull().sum()` and `describe()`) reveals the \"Dirty 20%\" we artificially injected:\n",
                "\n",
                "1.  **Missing Values:** `name` and `salary` fields have NULLs.\n",
                "2.  **Inconsistent Casing:** Job titles like \"software engineer\" mixed with \"Software Engineer\".\n",
                "3.  **Logic Errors:** Dates way in the past (<2015) or future (>2024).\n",
                "\n",
                "**Cleaning Strategy:**\n",
                "-   **Imputation:** We will start by filling missing salaries with the median salary of their respective positions.\n",
                "-   **Standardization:** We will convert all `position` strings to Title Case.\n",
                "-   **Handling Logic Errors:** We'll filter or correct the dates.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Identify Missing Values\n",
                "print(\"--- Missing Values Before Cleaning ---\")\n",
                "print(df.isnull().sum())\n",
                "\n",
                "# 2. Standardize Job Titles (to ensure grouping works for imputation)\n",
                "df['position'] = df['position'].str.title()\n",
                "print(\"\\n--- Job Titles Standardized ---\")\n",
                "\n",
                "# 3. Impute Missing Salaries (Position-based Median)\n",
                "# Calculate median salary per position\n",
                "position_medians = df.groupby('position')['salary'].transform('median')\n",
                "# Fill NaNs\n",
                "df['salary'] = df['salary'].fillna(position_medians)\n",
                "\n",
                "# Fill missing names (Optional, but good for completeness)\n",
                "df['name'] = df['name'].fillna(\"Unknown Employee\")\n",
                "\n",
                "# Verify cleaning\n",
                "print(\"\\n--- Missing Values After Cleaning ---\")\n",
                "print(df.isnull().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Transformation & Feature Engineering\n",
                "We need to extract meaningful features from the raw data. \n",
                "- **Start Year**: Extracted from the `start_date`.\n",
                "- **Years of Service**: Calculated as effectively `2024 - start_year` (or dynamic based on current date).\n",
                "- **Logic Filters**: Removing entries with invalid years (<2015 or >2024)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert start_date to datetime\n",
                "df['start_date'] = pd.to_datetime(df['start_date'])\n",
                "\n",
                "# Feature Engineering\n",
                "df['start_year'] = df['start_date'].dt.year\n",
                "current_year = 2024 # Fixed reference year for the lab\n",
                "df['years_of_service'] = current_year - df['start_year']\n",
                "\n",
                "# Logic Check: Filter out invalid years\n",
                "valid_years_mask = (df['start_year'] >= 2015) & (df['start_year'] <= 2024)\n",
                "df_clean = df[valid_years_mask].copy()\n",
                "\n",
                "print(f\"Rows before logic filter: {len(df)}\")\n",
                "print(f\"Rows after logic filter: {len(df_clean)}\")\n",
                "\n",
                "df_clean.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Neural Scaling (Z-Score Standardization)\n",
                "**Why Z-Score?**\n",
                "In Machine Learning, especially for Neural Networks (\"Neural Engines\"), input features must be on a similar scale. Large magnitude values (like Salary: 80,000) can dominate small magnitude values (like Years of Experience: 5), causing weight bias and slow convergence. \n",
                "\n",
                "Z-score standardization transforms the data to have a mean of 0 and a standard deviation of 1, ensuring the model treats all features equally.\n",
                "\n",
                "$$ z = \\frac{x - \\mu}{\\sigma} $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "scaler = StandardScaler()\n",
                "df_clean['salary_scaled'] = scaler.fit_transform(df_clean[['salary']])\n",
                "\n",
                "# Visualize the effect of scaling\n",
                "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
                "sns.histplot(df_clean['salary'], kde=True, ax=ax[0], color='skyblue').set_title('Original Salary Distribution')\n",
                "sns.histplot(df_clean['salary_scaled'], kde=True, ax=ax[1], color='orange').set_title('Scaled Salary (Z-Score) Distribution')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Visual Intelligence Challenge"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Grouped Bar Chart of Average Salary by Position\n",
                "plt.figure(figsize=(14, 8))\n",
                "# Calculate avg salary by Position and Start Year\n",
                "grouped_data = df_clean.groupby(['position', 'start_year'])['salary'].mean().reset_index()\n",
                "\n",
                "# Filter for top positions to keep chart readable if needed, or plot all\n",
                "top_positions = df_clean['position'].value_counts().nlargest(5).index\n",
                "grouped_data_filtered = grouped_data[grouped_data['position'].isin(top_positions)]\n",
                "\n",
                "sns.barplot(data=grouped_data, x='start_year', y='salary', hue='position', palette='viridis')\n",
                "plt.title('Average Salary by Position and Start Year', fontsize=16)\n",
                "plt.xlabel('Start Year', fontsize=12)\n",
                "plt.ylabel('Average Salary ($)', fontsize=12)\n",
                "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Advanced Challenge: Departmental Analytics\n",
                "Synthesizing a `departments` table and performing a SQL-style join."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate Synthetic Departments Table\n",
                "dept_data = {\n",
                "    'dept_id': [101, 102, 103, 104, 105],\n",
                "    'dept_name': ['Engineering', 'Data Science', 'Product', 'Sales', 'HR'],\n",
                "    'budget': [5000000, 3000000, 2000000, 4000000, 1000000]\n",
                "}\n",
                "departments = pd.DataFrame(dept_data)\n",
                "\n",
                "# Map positions to departments for joining\n",
                "# We need a common key. Let's create a mapping.\n",
                "position_dept_map = {\n",
                "    'Software Engineer': 'Engineering',\n",
                "    'Devops Engineer': 'Engineering',\n",
                "    'System Admin': 'Engineering',\n",
                "    'Data Scientist': 'Data Science',\n",
                "    'Qa Analyst': 'Engineering',\n",
                "    'Product Owner': 'Product',\n",
                "    'Project Manager': 'Product',\n",
                "    'Sales Associate': 'Sales',\n",
                "    'Marketing Manager': 'Sales',\n",
                "    'Hr Specialist': 'HR'\n",
                "}\n",
                "\n",
                "# Apply mapping to create a key in df_clean (handling title casing match)\n",
                "df_clean['dept_name'] = df_clean['position'].map(position_dept_map)\n",
                "\n",
                "# Handling unmapped positions just in case\n",
                "df_clean['dept_name'] = df_clean['dept_name'].fillna('Engineering')\n",
                "\n",
                "# SQL-style Join (Inner Join)\n",
                "merged_df = pd.merge(df_clean, departments, on='dept_name', how='inner')\n",
                "\n",
                "print(f\"Merged Data Shape: {merged_df.shape}\")\n",
                "merged_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Advanced Viz: FacetGrid showing Salary Distribution by Department\n",
                "g = sns.FacetGrid(merged_df, col=\"dept_name\", col_wrap=3, height=4, aspect=1.2, sharex=False)\n",
                "g.map(sns.histplot, \"salary\", kde=True, color=\"teal\")\n",
                "g.set_titles(\"{col_name}\")\n",
                "g.fig.suptitle('Salary Distribution by Department', y=1.02, fontsize=16)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Insights & Conclusions\n",
                "\n",
                "1.  **Salary Trends**: Through the grouped bar chart, we can observe salary progressions for different positions over time. Generally, positions like \"Data Scientist\" and \"Software Engineer\" command higher starting salaries.\n",
                "2.  **Data Quality Impact**: The initial \"dirty\" data (missing values, bad dates) required significant cleaning. Without imputation and logic filtering, our analysis would have been skewed by zero values or incorrect years.\n",
                "3.  **Departmental Structure**: The generated departmental view confirms that Engineering and Data Science consume the largest portion of the salary mass, consistent with the tech-heavy nature of the roles simulated.\n",
                "4.  **Scaling Necessity**: The Z-score visualization highlights how raw salary numbers (50k-150k) are transformed into a compact range (-2 to +2), which is essential for any downstream AI modeling."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}