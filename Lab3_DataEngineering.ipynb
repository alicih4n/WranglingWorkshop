{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 3: Data Engineering & EDA with Python, SQL, and Pandas\n",
                "\n",
                "**Student Name:** Ali Cihan Ozdemir\n",
                "**Student ID:** 9091405\n",
                "**Group Partner:** Roshan\n",
                "\n",
                "## Objective\n",
                "This notebook demonstrates a complete data engineering pipeline: connecting to a cloud PostgreSQL database (Neon), extracting raw \"dirty\" data, cleaning and transforming it using Pandas, and performing advanced exploratory data analysis (EDA) and visualization.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Data Extraction & Inspection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import psycopg2\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sqlalchemy import create_engine\n",
                "\n",
                "# Set aesthetic style\n",
                "sns.set_style(\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "\n",
                "# Database Configuration\n",
                "DB_URL = \"postgresql://neondb_owner:npg_a2cfwEmDp5ig@ep-noisy-grass-ai9zgc4l-pooler.c-4.us-east-1.aws.neon.tech/neondb?sslmode=require\"\n",
                "\n",
                "# Load the data from two separate tables\n",
                "def get_table_data(table_name):\n",
                "    try:\n",
                "        conn = psycopg2.connect(DB_URL)\n",
                "        query = f\"SELECT * FROM {table_name}\"\n",
                "        df = pd.read_sql_query(query, conn)\n",
                "        conn.close()\n",
                "        return df\n",
                "    except Exception as e:\n",
                "        print(f\"Error fetching {table_name}: {e}\")\n",
                "        return pd.DataFrame()\n",
                "\n",
                "df_raw = get_table_data('employees')\n",
                "df_dept = get_table_data('departments')\n",
                "\n",
                "df = df_raw.copy()\n",
                "print(f\"Employees Data Loaded: {df.shape}\")\n",
                "print(f\"Departments Data Loaded: {df_dept.shape}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Data Cleaning & Wrangling\n",
                "\n",
                "### Data Collection & Cleaning Narrative\n",
                "Our database now contains two related tables: `employees` and `departments`. The `employees` table has a Foreign Key `dept_id` linking to the `departments` table. We still observe the \"Dirty 20%\" issues we simulated:\n",
                "\n",
                "1.  **Missing Values:** `name` and `salary` fields have NULLs.\n",
                "2.  **Inconsistent Casing:** Job titles like \"software engineer\" mixed with \"Software Engineer\".\n",
                "3.  **Logic Errors:** Dates way in the past (<2015) or future (>2024).\n",
                "\n",
                "**Cleaning Strategy:**\n",
                "-   **Imputation:** We will start by filling missing salaries with the median salary of their respective positions.\n",
                "-   **Standardization:** We will convert all `position` strings to Title Case.\n",
                "-   **Handling Logic Errors:** We'll filter or correct the dates.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Identify Missing Values\n",
                "print(\"--- Missing Values Before Cleaning ---\")\n",
                "print(df.isnull().sum())\n",
                "\n",
                "# 2. Standardize Job Titles (to ensure grouping works for imputation)\n",
                "df['position'] = df['position'].str.title()\n",
                "print(\"\\n--- Job Titles Standardized ---\")\n",
                "\n",
                "# 3. Impute Missing Salaries (Position-based Median)\n",
                "# Calculate median salary per position\n",
                "position_medians = df.groupby('position')['salary'].transform('median')\n",
                "# Fill NaNs\n",
                "df['salary'] = df['salary'].fillna(position_medians)\n",
                "\n",
                "# Fill missing names (Optional, but good for completeness)\n",
                "df['name'] = df['name'].fillna(\"Unknown Employee\")\n",
                "\n",
                "# Verify cleaning\n",
                "print(\"\\n--- Missing Values After Cleaning ---\")\n",
                "print(df.isnull().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Transformation & Feature Engineering\n",
                "We need to extract meaningful features from the raw data. \n",
                "- **Start Year**: Extracted from the `start_date`.\n",
                "- **Years of Service**: Calculated as effectively `2024 - start_year` (or dynamic based on current date).\n",
                "- **Logic Filters**: Removing entries with invalid years (<2015 or >2024)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert start_date to datetime\n",
                "df['start_date'] = pd.to_datetime(df['start_date'])\n",
                "\n",
                "# Feature Engineering\n",
                "df['start_year'] = df['start_date'].dt.year\n",
                "current_year = 2024 # Fixed reference year for the lab\n",
                "df['years_of_service'] = current_year - df['start_year']\n",
                "\n",
                "# Logic Check: Filter out invalid years\n",
                "valid_years_mask = (df['start_year'] >= 2015) & (df['start_year'] <= 2024)\n",
                "df_clean = df[valid_years_mask].copy()\n",
                "\n",
                "print(f\"Rows before logic filter: {len(df)}\")\n",
                "print(f\"Rows after logic filter: {len(df_clean)}\")\n",
                "\n",
                "df_clean.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Neural Scaling (Z-Score Standardization)\n",
                "**Why Z-Score?**\n",
                "In Machine Learning, especially for Neural Networks (\"Neural Engines\"), input features must be on a similar scale. Large magnitude values (like Salary: 80,000) can dominate small magnitude values (like Years of Experience: 5), causing weight bias and slow convergence. \n",
                "\n",
                "Z-score standardization transforms the data to have a mean of 0 and a standard deviation of 1, ensuring the model treats all features equally.\n",
                "\n",
                "$$ z = \\frac{x - \\mu}{\\sigma} $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "scaler = StandardScaler()\n",
                "df_clean['salary_scaled'] = scaler.fit_transform(df_clean[['salary']])\n",
                "\n",
                "# Visualize the effect of scaling\n",
                "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
                "sns.histplot(df_clean['salary'], kde=True, ax=ax[0], color='skyblue').set_title('Original Salary Distribution')\n",
                "sns.histplot(df_clean['salary_scaled'], kde=True, ax=ax[1], color='orange').set_title('Scaled Salary (Z-Score) Distribution')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Visual Intelligence Challenge"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Grouped Bar Chart of Average Salary by Position\n",
                "plt.figure(figsize=(14, 8))\n",
                "# Calculate avg salary by Position and Start Year\n",
                "grouped_data = df_clean.groupby(['position', 'start_year'])['salary'].mean().reset_index()\n",
                "\n",
                "# Filter for top positions to keep chart readable if needed, or plot all\n",
                "top_positions = df_clean['position'].value_counts().nlargest(5).index\n",
                "grouped_data_filtered = grouped_data[grouped_data['position'].isin(top_positions)]\n",
                "\n",
                "sns.barplot(data=grouped_data, x='start_year', y='salary', hue='position', palette='viridis')\n",
                "plt.title('Average Salary by Position and Start Year', fontsize=16)\n",
                "plt.xlabel('Start Year', fontsize=12)\n",
                "plt.ylabel('Average Salary ($)', fontsize=12)\n",
                "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Advanced Challenge: Departmental Analytics\n",
                "Since we fetched the `departments` table separately from the database, we effectively have the raw components to perform a **SQL-Style Join** right here in Pandas. This mimics the Data Engineering process of enriching a transactional table (`employees`) with a dimension table (`departments`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform SQL-style Join (Inner Join)\n",
                "# employees.dept_id <---> departments.dept_id\n",
                "merged_df = pd.merge(df_clean, df_dept, on='dept_id', how='inner')\n",
                "\n",
                "print(f\"Merged Data Shape: {merged_df.shape}\")\n",
                "print(\"Columns:\", merged_df.columns.tolist())\n",
                "merged_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Advanced Viz: FacetGrid showing Salary Distribution by Department\n",
                "g = sns.FacetGrid(merged_df, col=\"dept_name\", col_wrap=3, height=4, aspect=1.2, sharex=False)\n",
                "g.map(sns.histplot, \"salary\", kde=True, color=\"teal\")\n",
                "g.set_titles(\"{col_name}\")\n",
                "g.fig.suptitle('Salary Distribution by Department', y=1.02, fontsize=16)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Insights & Conclusions\n",
                "\n",
                "1.  **Salary Trends**: Through the grouped bar chart, we can observe salary progressions for different positions over time. Generally, positions like \"Data Scientist\" and \"Software Engineer\" command higher starting salaries.\n",
                "2.  **Data Quality Impact**: The initial \"dirty\" data (missing values, bad dates) required significant cleaning. Without imputation and logic filtering, our analysis would have been skewed by zero values or incorrect years.\n",
                "3.  **Key Relationship Integration**: By joining the `employees` table with the `departments` table via `dept_id`, we successfully unpacked the department names (Engineering, Sales, etc.), proving that the data architecture is sound.\n",
                "4.  **Scaling Necessity**: The Z-score visualization highlights how raw salary numbers (50k-150k) are transformed into a compact range (-2 to +2), which is essential for any downstream AI modeling."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}